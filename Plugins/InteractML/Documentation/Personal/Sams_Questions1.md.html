<meta charset="utf-8" emacsmode="-*- markdown -*-"> <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">
                        **ML Questions 1**
					Sam Swain - 19/5/2021

After a pretty intense dive into the code and concepts of InteractML (and a bit of a crash course in ML) I had all these questions buzzing round my head on Friday evening.  Please excuse (and correct) any ML ignorance, practical systems like this are fairly new to me.

# Normalisation

1. How much data normalisation is needed? e.g. size of person affects the values coming in, resolution of display affects coordinate value ranges in a gesture.
2. Do we need to provide operators for this?
3. Does ML care about absolute offsets, or does this have to be trained for? e.g. does it matter where the person is standing when they perform a gesture, or which way they are facing? This sounds analogous to face recognition; is the 'finding the face in the image' part of the model or a separate system (another model?) to recognising who it is?

# System design

4. How much of the design of a system is/should be front-end pre-processing of data? e.g. selecting which input sources to train on, re-mappng them, normalising them, etc.
5. Shouldn't ML be able to apply in any situation? Or is this an efficiency question to reduce the need for training variants/operating costs? e.g. I want to match an arm gesture, so rather than train the same arm gesture with lots of different leg movements to reduce their relevance, I just filter out the leg data.
6. Isn't this just moving between ML and an expert system? or are there criteria for deciding which is the best tool for each task?

# Re-training

7. How much re-training is likely? e.g. I've changed the pre-processing and now I need to retrain the model(s).
8. Is this always a live-action task? Why not always carry it out via an intermediate record/playback stage?
9. This is exactly the same model as motion-capture and animation, can't we just use the same systems/tools/data formats to store/manage our parameter streams?
10. If a model has to be re-trained if the input types/list change, shouldn't we always capture parameters liberally to allow retraining? i.e. record as much raw data as possible. Then training can be run (and re-run) from these recordings as we explore changes to the pre-processing and selection stages.
11. Is the only benefit of recording training inputs that we can add more examples later and re-train? This points towards a shift of the recording of training examples back from after the pre-processing to before the preprocessing, i.e. Should we be recording the source parameters and not the data going into the model.
12. Will we want to be able to automate re-training passes? If you have a large corpus of parameter recordings and need to change a mapping or filtering stage it's tedious and time consuming re-train all the models manually. This also indicates that data management is going to be an important part of the workflow.

# DTW

13. To run a DTW model, currently you need to record 'N samples' between a start and end time. Would it be better to retrospectively trigger a match against some 'last N samples' from an ongoing recording?
14. How would we know how many samples to pass?
15. How does DTW deal with multiple labels that have different length examples?
16. Do we have to have separate models for 'gestures' of drastically differing length? What are the thresholds here?
17. Is it possible to match continuously (regularly) against a rolling window of captured input samples? Is this effective, or is there some other approach that makes more sense?
18. Can the DTW algorithms run 'progressively' or are they 'holistic'? i.e. Can we stream samples to the model to reduce match latency?
19. Is DTW time dependant? What if I train at 120Hz but my model is fed data at 60Hz?
20. How do we correct for this? Can we 'just' include time as a parameter? What about offsets? Do we have to time-stretch to match the training rate, or remap so both training and modelling are performed at samples provided at some standard rate?

# Models

21. Should we be able to leave models running whilst we are training new versions? Does this provide useful feedback?
22. (a) Is it expected that ML systems built with these tools are likely to include multiple ML stages chained together? e.g. train DTW on results of various regression models fed with mo-cap data to spot 'sequence of poses'?  (b) Presumably we need to support re-use? i.e. placing of the same model into different systems/scenes/actors?
23. Is training and retraining these sorts of systems going to be unmanageable without recording and playback? (see earlier questions).
24. Is visualisation of parameters (in and out), and tools to explore them going to become essential in the future?

# Labels

25. Labels (I think this is the term) seem to be just numbers, are they arbitrary or do the absolute/relative values have any effect on the operation of the model?
26. Can we map these numbers to arbitrary data types so that they are hidden from the user, e.g. text, enums, bool, etc?

# Sessions

27. In a live session with an appliction built around InteractML, any new training data (i.e. training in-app) will need to be stored, and then training re-run on it so the models get updated. How are we going to manage this data?
28. What concepts apply to the sort of workflow expected? e.g. Session, Recording, Set-of-recordings, Model, Partitions.
29. What versioning does all this need? e.g. “I've re-recorded the arm gestures again, so now we have three sets to choose from, see how you get on and which works best”
30. What API/tools/operations do we need to support this? (edited) 





<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>